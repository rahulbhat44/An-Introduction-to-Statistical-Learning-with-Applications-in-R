---
title: "Auto9.7"
author: "Rahul Bhat"
date: "6/6/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#######################################################################################
#Exercise 9.7
####In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.
######################################################################################

```{r}
library(ISLR)
data(Auto)
summary(Auto)
```

####(a). Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.

```{r}
med = median(Auto$mpg)
bin.var = ifelse(Auto$mpg > med, 1, 0)
Auto$mpglevel = as.factor(bin.var)
Auto$mpglevel
```

#### (b). Fit a support vector classifier to the data with various values of “cost”, in order to predict whether a car gets high of low gas mileage. Report the cross-validation errors associated with different values of this parameter.

```{r}
#The e1071 library includes a built-in function, tune(), to perform cross- validation. By default, tune() performs ten-fold cross-validation on a set of models of interest.
set.seed(34567)
library(e1071)
```

```{r}
#The following command indicates that we want to compare SVMs with a linear kernel, using a range of values of the cost parameter.
tune.fit = tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", 
                ranges = list(cost = c(0.5, 1, 5, 10, 25, 50, 100)))
```

```{r}
#We can easily access the cross-validation errors for each of these models using the summary() command.
summary(tune.fit)
# #We see that cost=1 results in the lowest cross-validation error rate. 
```

```{r}
#The tune() function stores the best model obtained, which can be accessed as follows.
bestmod=tune.fit$best.model 
summary(bestmod)
```
####(c). Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of “gamma” and “degree” and “cost”. 
```{r}
tune.fit1 <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", 
           ranges = list(cost = c(0.1, 1, 5, 10, 25, 50, 100), degree = c(2, 3, 4)))
summary(tune.fit1)
# lowest cross-validation error is obtained for cost = 100 and degree = 2
```

```{r}
tune.fit2 <- tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", 
            ranges = list(cost = c(0.1, 1, 5, 10), 
                    gamma = c(0.01, 0.1, 1, 5, 10, 100)))

summary(tune.fit2)
#For radial lowest cross-validation error is obtained for cost = 10 and gamma = 0.01
```
#### (d). Make some plots to back up your assertions in (b) and (c).
```{r}
svm.linear <- svm(mpglevel ~ ., data = Auto, kernel = "linear", cost = 1)
svm.polynomial <- svm(mpglevel ~ ., data = Auto, kernel = "polynomial", cost = 100, degree = 2)
svm.radial <- svm(mpglevel ~ ., data = Auto, kernel = "radial", cost = 100, gamma = 0.01)
plotpairs = function(fit) {
  for (name in names(Auto)[!(names(Auto) %in% c("mpg", "mpglevel", "name"))]) {
    plot(fit, Auto, as.formula(paste("mpg ~", name, sep = "")))
  }
}
plotpairs(svm.linear)

plotpairs(svm.polynomial)

plotpairs(svm.radial)
```

